{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb70b71d-744c-47ab-a04b-d2d602cac903",
   "metadata": {},
   "source": [
    "<b>Español</b>\n",
    "\n",
    "Este cuaderno está dedicado al estudio de la volatilidad en instrumentos financieros. Estos instrumentos financieros son aquellos cuyos subyacentes constituyen la base de otro instrumento financiero, como las opciones. Los subyacentes estudiados incluirán activos como criptomonedas (en este caso, Bitcoin), acciones (por ejemplo, acciones de índices como el NASDAQ, S&P 500 u otros), materias primas u otros tipos de activos.\n",
    "\n",
    "En primer lugar, recopilaremos una cierta cantidad de datos sobre la actividad del activo financiero. Estos datos podrán ser en unidades de tiempo como horas, minutos, días o cualquier otra medida que se considere apropiada. Posteriormente, procederemos a analizar la distribución de la volatilidad observada, determinando si sigue una distribución específica (como normal, gamma, u otra) o si presenta características aleatorias. Una vez que se establezca el tipo de distribución, podremos calcular la probabilidad de que la volatilidad se sitúe dentro de uno u otro rango. Estos rangos se definirán como baja volatilidad, volatilidad media o alta volatilidad.\n",
    "\n",
    "Además, analizaremos el clustering de la volatilidad, es decir, si periodos de baja volatilidad tienden a ser seguidos por otros de baja volatilidad, y lo mismo para periodos de alta volatilidad. Este análisis nos permitirá implementar modelos de predicción de volatilidad, como los modelos ARCH, GARCH u otros similares.\n",
    "\n",
    "Estos modelos nos proporcionarán la capacidad de predecir la volatilidad implícita o la volatilidad futura del activo financiero en cuestión. Con los resultados obtenidos, los modelos se podrán aplicar principalmente en estrategias de valoración o en operaciones de compra y venta de opciones, aunque también pueden adaptarse a otras estrategias de mercado.\n",
    "\n",
    "Un ejemplo de estas estrategias es el Butterfly Spread, que implica la compra y venta de opciones call. Este enfoque apuesta por la posibilidad de que, tras una disminución de la actividad, el precio del activo se mantenga dentro de un rango específico. Si el precio permanece dentro de ese rango, la estrategia será rentable. Por el contrario, si el precio se desvía significativamente del rango, la estrategia no generará beneficios.\n",
    "\n",
    "Todas las explicaciones y comentarios del código estarán en inglés.\n",
    "\n",
    "---\n",
    "\n",
    "<b>English</b>\n",
    "\n",
    "This notebook is dedicated to studying volatility in financial instruments. These financial instruments are those whose underlying assets form the basis of another financial instrument, such as options. The underlying assets analyzed will include cryptocurrencies (in this case, Bitcoin), stocks (e.g., shares from indices like NASDAQ, S&P 500, or others), commodities, or other types of assets.\n",
    "\n",
    "First, we will gather a dataset on the financial activity of the asset. The data may be collected in time intervals such as hours, minutes, days, or any other preferred measure. Subsequently, we will analyze the distribution of the observed volatility to determine whether it follows a specific distribution (such as normal, gamma, or another) or if it exhibits random characteristics. Once the type of distribution is established, we can calculate the probability of volatility falling within certain ranges. These ranges will be classified as low volatility, medium volatility, or high volatility.\n",
    "\n",
    "Additionally, we will examine volatility clustering, i.e., whether periods of low volatility tend to be followed by other low-volatility periods and whether high-volatility periods show similar patterns. This analysis will allow us to implement volatility prediction models, such as ARCH, GARCH, or similar frameworks.\n",
    "\n",
    "These models will enable us to predict the implied volatility or future volatility of the financial asset being assessed. Once the volatility results are obtained, the models can be primarily applied in valuation strategies or in options trading, though they may also be adapted for other market strategies.\n",
    "\n",
    "An example of these strategies is the Butterfly Spread, which involves buying and selling call options. This strategy assumes that, following a decrease in market activity, the asset price will remain within a specific range. If the price stays within this range, the strategy will be profitable. Conversely, if the price deviates significantly from the range, the strategy will not yield profits.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342fb911-8008-4e37-b96d-b961b63645d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every important library we are gonna use (or not but is usefull to have as a resource)\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import websocket\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf \n",
    "from scipy.stats import shapiro, normaltest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bef1eb-b039-475f-829e-c15b07b8b055",
   "metadata": {},
   "source": [
    "The code will be in parts, usually there will be an explanation and examples of reasoning, then the working code.<br> I will diferenciate the explanation cells with #E and the working cells with #W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3535ca87-1693-446c-aa20-1b5d7fb60bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "now = time.time() * 1000\n",
    "start_date = datetime.datetime(2017, 8, 18) #It's the first available data from this source\n",
    "startTime_0 = int(start_date.timestamp()*1000)\n",
    "\n",
    "#As the API connection has a limit on how many requests per second, we are gonna divide dinamically the request into as few bunchs as possible.\n",
    "time_diff = now - startTime_0\n",
    "n_hours_diff = int(time_diff / (1000 * 60 * 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90b81887-c661-4bd0-92c6-7836de59e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "url = 'https://data-api.binance.vision/api/v3/klines'\n",
    "\n",
    "#The limit is how many intervals per request, as we got 1000 in this API, we got n_requests.\n",
    "symbol = 'BTCUSDT'\n",
    "interval = '1h'\n",
    "startTimes = [startTime_0]\n",
    "limit = 1000\n",
    "\n",
    "#Keep in mind the number of request will go up by one every 1000 hours (41 days and 16 hours).\n",
    "n_requests = math.floor(n_hours_diff / limit)\n",
    "pause_count = 0 #We'll count how many requests without a pause, and, if needed, pause the code for x seconds.\n",
    "time_interval = 1000 * 60 * 60 * 1000 #Interval of 1000 hours, will be used to make multiple startTime_i\n",
    "\n",
    "#This API has a weight of 2, and a limit of 1200, so we have 600 requests per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8e6310-5648-47ad-963f-37b03dc6f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "data_example =[\n",
    "                [\n",
    "                    1499040000000,      #// Kline open time\n",
    "                    \"0.01634790\",       #// Open price\n",
    "                    \"0.80000000\",       #// High price\n",
    "                    \"0.01575800\",       #// Low price\n",
    "                    \"0.01577100\",       #// Close price\n",
    "                    \"148976.11427815\",  #// Volume\n",
    "                    1499644799999,      #// Kline Close time\n",
    "                    \"2434.19055334\",    #// Quote asset volume\n",
    "                    308,                #// Number of trades\n",
    "                    \"1756.87402397\",    #// Taker buy base asset volume\n",
    "                    \"28.46694368\",      #// Taker buy quote asset volume\n",
    "                    \"0\"                 #// Unused field, ignore.\n",
    "                  ]\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fd5f7-569d-4f13-b33f-4fea6ccd28df",
   "metadata": {},
   "source": [
    "From this data, we are interested in the first 6 values, from the open time to the volume (included), an will get a list for every one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8440a78-f879-4364-bb8f-a2426e637a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "Time, Open, High, Low, Close, Volume = [], [], [], [], [], []\n",
    "\n",
    "response = requests.get(url, params = {'symbol' : symbol, 'interval' : interval, 'startTime' : startTime_0, 'limit' : limit})\n",
    "initial_data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7895c319-f329-47b4-bd79-215c6d8b88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503007200000 1506625200000\n"
     ]
    }
   ],
   "source": [
    "#E\n",
    "print(initial_data[0][0],initial_data[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e51775-d7fb-4db4-9bc0-f1093f3e945d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618000000 3600000000\n"
     ]
    }
   ],
   "source": [
    "#E\n",
    "hours_diff = initial_data[-1][0] - initial_data[0][0]\n",
    "\n",
    "print(hours_diff, time_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4bad30-49bd-4083-8b41-d431241eb509",
   "metadata": {},
   "source": [
    "As we see, the diference between the first and last hour of data is the 3618000000, more than we have calculated before, and that is because we are getting 5 hours more of data by default, so we have to adjust to it. We adjust by adding the hours_dif and one hour to the startTime_{i-1}, because if we don't add that hour, the last and first hour of consecutive requests is gonna be the same hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efb073d5-d4ce-41f2-a089-1f161bfbcacc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(17164 bytes read, 35606 more expected)', IncompleteRead(17164 bytes read, 35606 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:748\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 748\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:894\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    885\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_content_length\n\u001b[0;32m    886\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[0;32m    893\u001b[0m         \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[1;32m--> 894\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_bytes_read, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining)\n\u001b[0;32m    895\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read1 \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    896\u001b[0m     (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m    897\u001b[0m ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;66;03m# `http.client.HTTPResponse`, so we close it here.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;66;03m# See https://github.com/python/cpython/issues/113199\u001b[39;00m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(17164 bytes read, 35606 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:872\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m    873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:772\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m         arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 772\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(arg, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(17164 bytes read, 35606 more expected)', IncompleteRead(17164 bytes read, 35606 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#E\u001b[39;00m\n\u001b[0;32m      2\u001b[0m startTime_1 \u001b[38;5;241m=\u001b[39m startTime_0 \u001b[38;5;241m+\u001b[39m hours_diff \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response_1 \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msymbol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minterval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstartTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartTime_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlimit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m initial_data_1 \u001b[38;5;241m=\u001b[39m response_1\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(initial_data_1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], initial_data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(17164 bytes read, 35606 more expected)', IncompleteRead(17164 bytes read, 35606 more expected))"
     ]
    }
   ],
   "source": [
    "#E\n",
    "startTime_1 = startTime_0 + hours_diff + 1000 * 60 * 60\n",
    "response_1 = requests.get(url, params = {'symbol' : symbol, 'interval' : interval, 'startTime' : startTime_1, 'limit' : limit})\n",
    "initial_data_1 = response_1.json()\n",
    "\n",
    "print(initial_data_1[0][0], initial_data[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72afc32-1fc7-44c4-b4e2-70dda5a68612",
   "metadata": {},
   "source": [
    "As we see, the first hour of the second request is the next hour from the last hour of the first request. (3.600.000 units in unix milisecond time) <br>\n",
    "Now lets get all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517a7e3-45ed-4f07-9a57-52fb982efe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "now = time.time() * 1000\n",
    "start_date = datetime.datetime(2017, 8, 18) #It's the first available data from this source\n",
    "startTime_0 = int(start_date.timestamp()*1000)\n",
    "\n",
    "#As the API connection has a limit on how many requests per second, we are gonna divide dinamically the request into as few bunchs as possible.\n",
    "time_diff = now - startTime_0\n",
    "n_hours_diff = int(time_diff / (1000 * 60 * 60))\n",
    "\n",
    "data_url = 'https://data-api.binance.vision/api/v3/klines'\n",
    "\n",
    "#The limit is how many intervals per request, as we got 1000 in this API, we got n_requests.\n",
    "symbol = 'BTCUSDT'\n",
    "interval = '1h'\n",
    "startTimes = [startTime_0]\n",
    "limit = 1000\n",
    "\n",
    "#Keep in mind the number of request will go up by one every 1000 hours (41 days and 16 hours).\n",
    "n_requests = math.floor(n_hours_diff / limit)\n",
    "pause_count = 0 #We'll count how many requests without a pause, and, if needed, pause the code for x seconds.\n",
    "time_interval = 1000 * 60 * 60 * 1005 #Interval of 1005 hours, will be used to make multiple startTime_{i}\n",
    "#This API has a weight of 2, and a limit of 1200, so we have 600 requests per minute.\n",
    "\n",
    "#The list for every startTime value\n",
    "for i in range(n_requests):\n",
    "    startTimes.append(startTimes[-1] + time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8919497-3efa-4332-b2f7-d7bb1c17e581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#W\n",
    "Time, Open, High, Low, Close, Volume = [], [], [], [], [], []\n",
    "\n",
    "def fetch_asset_data(url, symbol, interval, startTimes, limit):\n",
    "    for startTime in startTimes:\n",
    "        params = {\n",
    "            'symbol' : symbol,\n",
    "            'interval' : interval,\n",
    "            'startTime' : startTime,\n",
    "            'limit' : limit\n",
    "        }\n",
    "    \n",
    "        response = requests.get(url, params)\n",
    "        data = response.json()\n",
    "    \n",
    "        for i in range(len(data)):\n",
    "            Time.append(float(data[i][0]))\n",
    "            Open.append(float(data[i][1]))\n",
    "            High.append(float(data[i][2]))\n",
    "            Low.append(float(data[i][3]))\n",
    "            Close.append(float(data[i][4]))\n",
    "            Volume.append(float(data[i][5]))\n",
    "\n",
    "    return Time, Open, High, Low, Close, Volume\n",
    "\n",
    "fetch_asset_data(data_url, symbol, interval, startTimes, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc09e5-7174-419b-b5ad-2d71aec75683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "data_dict = {\n",
    "    'Time' : Time,\n",
    "    'Open' : Open,\n",
    "    'High' : High,\n",
    "    'Low' : Low,\n",
    "    'Close' : Close,\n",
    "    'Volume' : Volume\n",
    "}\n",
    "\n",
    "with open('BTC data from Binance.json', 'w') as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4accb9e-ed7a-4956-87aa-ad72b26832aa",
   "metadata": {},
   "source": [
    "Now that we have 63.000+ points of data is when we are gonna start the analysis, basically analyse the vol (volatility) distribution, create models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6136fda-afc5-406e-9c7f-03e969ba8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "with open('BTC data from Binance.json', 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "    #If needed in between sessions\n",
    "\n",
    "Time = loaded_data['Time']\n",
    "Open = loaded_data['Open']\n",
    "High = loaded_data['High']\n",
    "Low = loaded_data['Low']\n",
    "Close = loaded_data['Close']\n",
    "Volume = loaded_data['Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7193a8-c715-4a96-b9b1-5dd34d6bffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "returns = [np.log(Close[i+1]/Close[i]) for i in range(len(Close)-1)]\n",
    "vol_factor = np.sqrt(24*365) #for annualised volatility\n",
    "vol =[]\n",
    "\n",
    "window_size = 24 #Volatility over how many hours\n",
    "\n",
    "for i in range(0, len(returns), window_size):\n",
    "    window = returns[i : i+window_size]\n",
    "    if len(window) == window_size:\n",
    "        volatility = np.std(window, ddof=1)\n",
    "        vol.append(volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a3a8b-131a-4a34-b2cf-72ad172934db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(returns, label=\"Returns\")\n",
    "plt.title(\"Returns\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(vol, label=\"Volatility\", color=\"orange\")\n",
    "plt.title(\"Volatility (24-hour)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfdb0a-6587-4a90-89f2-bef31674c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot autocorrelation of squared returns (proxy for volatility)\n",
    "plot_acf(np.square(returns), lags=100, title=\"Autocorrelation of Squared Returns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a70456-8dd6-4f83-8b8c-ad898f43ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "mean_vol = np.mean(vol)\n",
    "std_vol = np.std(vol, ddof=1)\n",
    "print(f\"Mean Volatility: {mean_vol:.4f}\")\n",
    "print(f\"Standard Deviation of Volatility: {std_vol:.4f}\")\n",
    "\n",
    "\n",
    "#Normality tests\n",
    "shapiro_test = shapiro(vol)\n",
    "dagostino_test = normaltest(vol)\n",
    "\n",
    "print(\"Shapiro-Wilk Test:\")\n",
    "print(f\"Statistic: {shapiro_test.statistic:.4f}, p-value: {shapiro_test.pvalue:.4f}\")\n",
    "print(\"D’Agostino and Pearson’s Test:\")\n",
    "print(f\"Statistic: {dagostino_test.statistic:.4f}, p-value: {dagostino_test.pvalue:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3f183-d41f-4652-a8c2-994f58ca1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "plt.hist(vol, bins=100)\n",
    "plt.title('Distribution of Volatility')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bcce3-e853-4949-8d10-f9b6b7d7024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "log_vol = []\n",
    "\n",
    "for volatility in vol:\n",
    "    if volatility != 0:\n",
    "        log_vol.append(np.log(volatility))\n",
    "    else:\n",
    "        log_vol.append(volatility)\n",
    "\n",
    "\n",
    "plt.hist(log_vol, bins=100)\n",
    "plt.title('Distribution of Logarithmic Volatility')\n",
    "plt.xlabel('Logarithmic Volatility')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad04bf-1126-4e99-9351-01913edcc897",
   "metadata": {},
   "source": [
    "<b>What are we working with?</b><br>\n",
    "\n",
    "We have defined volatility as the standard deviation of returns over a given period of time, in this case, 24 hours. The first thing we observed when plotting both returns and volatility is that both seem to exhibit clustering. This was confirmed through the autocorrelation plot of squared returns, although it’s worth noting that this is just one measure; other metrics can be used, and autocorrelations can be analyzed similarly.\n",
    "\n",
    "Next, we examined the mean and standard deviation of the volatility itself. Here, we observed that while the returns are already centered, the standard deviation of the volatility is significantly high compared to the mean. However, this mean is not excessively high when considering the market it pertains to, though it remains noteworthy.\n",
    "\n",
    "Additionally, using the Shapiro-Wilk and D’Agostino tests, we confirmed that volatility does not follow a normal distribution. When plotting both the volatility and its logarithm, we found that the logarithm of volatility follows a log-normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "<b>The Butterfly Spread Strategy</b><br>\n",
    "This strategy involves using three different strike prices: \n",
    "$K_1$, $K_2$ and $K_3$. Specifically:\n",
    "\n",
    "Buy one call at strike price $K_1$: This gives you the right to buy the underlying asset at $K_1$<br>\n",
    "Sell two calls at strike price $K_2$: You take on the obligation to sell the underlying asset at $K_2$, earning a premium for each call sold.<br>\n",
    "Buy one call at strike price $K_3$: You purchase a right to buy the underlying asset at $K_3$, typically further out-of-the-money.<br>\n",
    "\n",
    "The result is a payoff structure where you profit if the underlying asset's price ends up in the interval $[K_1;K_3]$, with the maximum profit occurring when the price is close to $K_2$.\n",
    "\n",
    "<b>Mechanics of Options</b><br>\n",
    "\n",
    "When you buy a call option, you pay a premium upfront. If the asset's price exceeds the strike price $K$ by more than the premium paid, you make a net profit. Otherwise, the option expires worthless, and your loss is limited to the premium.\n",
    "\n",
    "When you sell a call option, you collect the premium upfront. If the asset’s price stays below the strike price $K$, the option expires worthless, and you keep the premium as profit. However, if the asset’s price rises above $K$, you are obligated to sell the asset at $K$, potentially incurring significant losses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4952b0-104e-4252-99d0-c46396620efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "Strike_price = 100\n",
    "Asset_price = np.linspace(75, 125, 51)\n",
    "Premium = 5\n",
    "\n",
    "P_L = []\n",
    "\n",
    "for i in Asset_price:\n",
    "    if i < Strike_price:\n",
    "        P_L.append(-Premium)\n",
    "    else:\n",
    "        Returns = i - Strike_price - Premium\n",
    "        P_L.append(Returns)\n",
    "\n",
    "plt.plot(Asset_price, P_L)\n",
    "plt.title('Evolution of a Call option (buyer)', fontweight = 'bold')\n",
    "plt.xlabel('Underlying asset price')\n",
    "plt.ylabel('Return of the trade')\n",
    "plt.ylim(-25,25)\n",
    "plt.xlim(min(Asset_price) - 5, max(Asset_price) + 5)\n",
    "plt.axhline(y = 0, color = 'red', linestyle = '--', linewidth = 0.75)\n",
    "plt.axvline(x = Strike_price, color = 'green', linestyle = '--', linewidth = 0.75)\n",
    "plt.axvline(x = (Strike_price + Premium), color = 'purple', linestyle = '--', linewidth = 0.75)\n",
    "plt.grid(color = 'grey', linestyle = '-', linewidth = 0.5, alpha = 0.25)\n",
    "plt.legend(['P/L curve', '0', 'Strike price', 'Breakeven price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6163c01-67e4-435d-8c20-05f988774392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "Strike_price = 100\n",
    "Asset_price = np.linspace(75, 125, 51)\n",
    "Premium = 5\n",
    "\n",
    "P_L = []\n",
    "\n",
    "for i in Asset_price:\n",
    "    if i < Strike_price:\n",
    "        P_L.append(Premium)\n",
    "    else:\n",
    "        Returns = i - Strike_price - Premium\n",
    "        P_L.append(-Returns)\n",
    "\n",
    "plt.plot(Asset_price, P_L)\n",
    "plt.title('Evolution of a Call option (seller)', fontweight = 'bold')\n",
    "plt.xlabel('Underlying asset price')\n",
    "plt.ylabel('Return of the trade')\n",
    "plt.ylim(-25,25)\n",
    "plt.xlim(min(Asset_price) - 5, max(Asset_price) + 5)\n",
    "plt.axhline(y = 0, color = 'red', linestyle = '--', linewidth = 0.75)\n",
    "plt.axvline(x = Strike_price, color = 'green', linestyle = '--', linewidth = 0.75)\n",
    "plt.axvline(x = (Strike_price + Premium), color = 'purple', linestyle = '--', linewidth = 0.75)\n",
    "plt.grid(color = 'grey', linestyle = '-', linewidth = 0.5, alpha = 0.25)\n",
    "plt.legend(['P/L curve', '0', 'Strike price', 'Breakeven price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16a446-bb33-46bb-9512-aebd8eb5cafb",
   "metadata": {},
   "source": [
    "As we see, the positions of the buyer and the seller are exactly the opposite, now let's do the full butterfly spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dedc3-e332-4ef1-9ca5-42e02cd74a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#E\n",
    "# Strike prices and premiums\n",
    "strike_1, premium_1 = 95, 10\n",
    "strike_2, premium_2 = 100, 5\n",
    "strike_3, premium_3 = 105, 2\n",
    "\n",
    "# Asset prices\n",
    "asset_price = np.linspace(85, 115, 51)\n",
    "\n",
    "# Calculate butterfly spread P/L\n",
    "butterfly_return = []\n",
    "\n",
    "for price in asset_price:\n",
    "    if price < strike_1:  # Below the first strike\n",
    "        single_return = -premium_1 + (2 * premium_2) - premium_3\n",
    "    elif strike_1 <= price <= strike_2:  # Between strike_1 and strike_2\n",
    "        single_return = (price - strike_1 - premium_1) + (2 * premium_2) - premium_3\n",
    "    elif strike_2 < price < strike_3:  # Between strike_2 and strike_3\n",
    "        single_return = (price - strike_1 - premium_1) + (2 * premium_2 - 2 * (price - strike_2)) - premium_3\n",
    "    else:  # Above the third strike\n",
    "        single_return = (price - strike_1 - premium_1) + (2 * premium_2 - 2 * (price - strike_2)) + (price - strike_3 - premium_3)\n",
    "    butterfly_return.append(single_return)\n",
    "\n",
    "# Net cost of the butterfly spread\n",
    "net_cost = premium_1 - 2 * premium_2 + premium_3\n",
    "\n",
    "# Break-even points\n",
    "breakeven_1 = strike_1 + net_cost\n",
    "breakeven_2 = strike_3 - net_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0247f19-cea5-4867-b178-4998234d6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(asset_price, butterfly_return)\n",
    "plt.title('Evolution of a Butterfly spread', fontweight = 'bold')\n",
    "plt.xlabel('Underlying asset price')\n",
    "plt.ylabel('Return of the trade')\n",
    "plt.axvline(x = breakeven_1, color = 'green', alpha = 0.30, linestyle = '--')\n",
    "plt.axvline(x = breakeven_2, color = 'green', alpha = 0.30, linestyle = '--')\n",
    "plt.axhline(y = 0, color = 'red', alpha = 0.10, linestyle = '--')\n",
    "plt.ylim(-5,5)\n",
    "plt.xlim(90,110)\n",
    "plt.grid(color = 'grey', linestyle = '-', linewidth = 0.5, alpha = 0.25)\n",
    "plt.legend(['P/L curve', 'Breakeven 1', 'Breakeven 2', '0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a5fc06-dae2-4da1-b0dd-802b3f25eda8",
   "metadata": {},
   "source": [
    "Please note that, even though the chart seems to be bigger than needed, the X-axis shows a $\\pm10$% in the underlying asset price, and the breakeven zone is in a $\\pm4$% range, which for short term operations is quite big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b184bac-04b6-4e36-a07d-406497ccda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "def fetch_options_data(url, currency, kind):\n",
    "    params = {\n",
    "        'currency' : currency,\n",
    "        'kind' : kind,\n",
    "        'expired' : 'true'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params)\n",
    "    options_data = response.json()\n",
    "\n",
    "    return options_data.get('result',[])\n",
    "\n",
    "def filter_options_data(options_data, target_date):\n",
    "    '''\n",
    "    Be **EXTREMELY** cautious on how is formated the data after getting it.\n",
    "    '''\n",
    "    start_of_day = int(time.mktime(time.strptime(target_date, \"%Y-%m-%d\")) * 1000)\n",
    "    end_of_day = start_of_day + (24 * 60 * 60 * 1000) - 1  # End of the day in milliseconds\n",
    "    filtered_data = [opt for opt in options_data if start_of_day <= opt[\"expiration_timestamp\"] <= end_of_day]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2fa056-f69e-4858-a296-c04babeaab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#W\n",
    "currency = 'BTC'\n",
    "kind = 'option'\n",
    "options_url = 'https://history.deribit.com/api/v2/public/get_instruments'\n",
    "\n",
    "options_data = fetch_options_data(options_url, currency, kind)\n",
    "\n",
    "with open('Options data from 15JUL2016-BTC.json', 'w') as f:\n",
    "    json.dump(options_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dee2e4-4bc2-405a-a7e5-4a5ea29aa215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "options_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7850e04-c076-4300-be4e-8500ba18f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#E\n",
    "target_date = '2024-11-22'\n",
    "\n",
    "filtered_options = filter_options_data(options_data, target_date)\n",
    "\n",
    "print(f\"Found {len(filtered_options)} options expiring on {target_date}.\")\n",
    "for opt in filtered_options[:1]:  # Show the first 5 for inspection\n",
    "    print(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54a88e-7a3f-42aa-9e10-4c46d4a2bf59",
   "metadata": {},
   "source": [
    "Up till here, I've just been demonstrating the data we have, how we will measure the strategy returns, the dates, and the evaluation, from now on the work will be done in the next areas, not by order:<br><br>\n",
    "- Define the strategy not in full butterfly spreads, but rather in a group of options; this mean I won't be neccesary operating with buying 1 Call, selling 2 Calls and buying another Call, but maybe adding one call or more in one group or another, reflecting how my views are at the moment, e.i. if I think that price is gonna increase even with low volatility, I may buy 2 Calls at the third strike rather than only one.<br>\n",
    "- Viability test; even before creating the model, I will take some periods with low vol (from the data of course), and check the price in t+1 (being t the moment that vol is low), and checking if the price is in the range we will define for the operation. (Between K1 and K3)<br>\n",
    "- Creating the model; here I don't have much to say, I will code a GARCH model to test it, and then by trial and error we will see the automated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479db53-83fc-43d6-8864-fdad3e2a112d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
