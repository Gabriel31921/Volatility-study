{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7e0807-820d-4160-9392-d344847c399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Every important library we are gonna use (or not but is usefull to have as a resource)\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import websocket\n",
    "import networkx as nx\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf \n",
    "from scipy.stats import shapiro, normaltest, levene\n",
    "from scipy.stats import chi2_contingency\n",
    "from datetime import timedelta, date\n",
    "from collections import defaultdict\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc99cab-0fac-4a72-8f18-28bd15911378",
   "metadata": {},
   "source": [
    "##### Fetching data and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9bc3d8-62c8-482c-b525-5228a69ac505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spot_data_binance(symbol, interval, startTimes, limit):\n",
    "    url = 'https://data-api.binance.vision/api/v3/klines'\n",
    "    \n",
    "    asset_data_binance = {\n",
    "        'Time' : [],\n",
    "        'Open' : [],\n",
    "    } \n",
    "    \n",
    "    for startTime in startTimes:\n",
    "        params = {\n",
    "            'symbol' : symbol,\n",
    "            'interval' : interval,\n",
    "            'startTime' : startTime,\n",
    "            'limit' : limit\n",
    "        }\n",
    "    \n",
    "        response = requests.get(url, params)\n",
    "        partial_data = response.json()\n",
    "    \n",
    "        for entry in partial_data:\n",
    "             asset_data_binance['Time'].append(float(entry[0]))\n",
    "             asset_data_binance['Open'].append(float(entry[1]))\n",
    "\n",
    "    return asset_data_binance\n",
    "\n",
    "def fetch_spot_data_bitget(symbol, granularity, endTimes, limit):\n",
    "    url = 'https://api.bitget.com/api/v2/spot/market/history-candles'\n",
    "\n",
    "    asset_data_bitget = {\n",
    "        'Time' : [],\n",
    "        'Open' : []\n",
    "    }\n",
    "    \n",
    "    pause_timer = 0    \n",
    "    for endTime in endTimes:\n",
    "        params = {\n",
    "                'symbol' : symbol,\n",
    "                'granularity' : granularity,\n",
    "                'endTime' : endTime,\n",
    "                'limit' : limit\n",
    "            }\n",
    "        pause_timer += 1\n",
    "        if pause_timer == 18:\n",
    "            time.sleep(0.5)\n",
    "            pause_timer = 0\n",
    "            \n",
    "        response = requests.get(url, params)\n",
    "        if response.status_code == 200:\n",
    "            pre_partial_data = response.json()\n",
    "            if 'data' in pre_partial_data:\n",
    "                partial_data = pre_partial_data['data']\n",
    "                for entry in partial_data:\n",
    "                    asset_data_bitget['Time'].append(float(entry[0]))\n",
    "                    asset_data_bitget['Open'].append(float(entry[1]))\n",
    "\n",
    "    return asset_data_bitget\n",
    "\n",
    "def fetch_options_data():\n",
    "    url = 'https://history.deribit.com/api/v2/public/get_instruments'\n",
    "    params = {\n",
    "        'currency' : 'BTC',\n",
    "        'kind' : 'option',\n",
    "        'expired' : 'true'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params)\n",
    "    options_data = response.json()\n",
    "\n",
    "    return options_data.get('result',[])\n",
    "\n",
    "def trades_per_instrument(instruments_name, start_timestamp, end_timestamp):\n",
    "    instruments_url = 'https://history.deribit.com/api/v2/public/get_last_trades_by_instrument_and_time'\n",
    "\n",
    "    instruments_trade = {instrument_name : [] for instrument_name in instruments_name}\n",
    "    \n",
    "    for instrument_name in instruments_name:\n",
    "        params = {\n",
    "            'instrument_name': instrument_name,\n",
    "            'start_timestamp': start_timestamp,\n",
    "            'end_timestamp': end_timestamp,\n",
    "            'count': 25,  # Adjust as needed\n",
    "            'sorting': 'asc'\n",
    "        }\n",
    "    \n",
    "        response = requests.get(instruments_url, params=params)\n",
    "        data = response.json()\n",
    "        trades = data.get('result', {}).get('trades', [])\n",
    "\n",
    "        for trade in trades:\n",
    "            instruments_trade['instrument_name'].append(trade)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161e97b-c7ea-48ae-887d-1e279a0a0c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params needed for fetching the spot data.\n",
    "endTime_0 = int(datetime.datetime(2024, 11, 1).timestamp() * 1000)\n",
    "start_date = int(datetime.datetime(2018, 8, 18).timestamp() * 1000)\n",
    "\n",
    "n_hours_to_get = int((endTime_0 - start_date) / (1000 * 60 * 60))\n",
    "\n",
    "symbol = 'BTCUSDT'\n",
    "interval = granularity = '1h'\n",
    "startTimes = [start_date]\n",
    "endTimes = [endTime_0] #At the end, this list has to have 270+ values, because there are 263 intervals of 200 hours between start_date and endTime_0 (at least)\n",
    "limit_binance = 1000\n",
    "limit_bitget = 200\n",
    "\n",
    "n_requests_binance = int(n_hours_to_get / limit_binance)\n",
    "n_requests_bitget = int(n_hours_to_get / limit_bitget)\n",
    "time_interval_binance = 60*60 * 1000 * limit_binance\n",
    "time_interval_bitget = 60*60 * 1000 * limit_bitget\n",
    "\n",
    "interval_to_ms = {\n",
    "    '1h': 60 * 60 * 1000,\n",
    "    '4h': 4 * 60 * 60 * 1000,\n",
    "    '6h': 6 * 60 * 60 * 1000,\n",
    "    '12h': 12 * 60 * 60 * 1000,\n",
    "    '1d': 24 * 60 * 60 * 1000,\n",
    "}\n",
    "\n",
    "asset_data = {}\n",
    "\n",
    "for i in range(n_requests_binance):\n",
    "    startTimes.append(startTimes[-1] + time_interval_binance)\n",
    "for i in range(n_requests_bitget):\n",
    "    endTimes.insert(0, endTimes[0] - time_interval_bitget)\n",
    "    \n",
    "\n",
    "asset_data_binance = fetch_spot_data_binance(symbol, interval, startTimes, limit_binance)\n",
    "asset_data_bitget = fetch_spot_data_bitget(symbol, granularity, endTimes, limit_bitget)\n",
    "\n",
    "initial_df_binance = pd.DataFrame(asset_data_binance)\n",
    "initial_df_bitget = pd.DataFrame(asset_data_bitget)\n",
    "\n",
    "initial_df_binance['Date'] = pd.to_datetime(initial_df_binance['Time'], unit = 'ms')\n",
    "initial_df_binance.set_index('Date', inplace = True)\n",
    "initial_df_binance = initial_df_binance.sort_index()\n",
    "\n",
    "initial_df_bitget['Date'] = pd.to_datetime(initial_df_bitget['Time'], unit = 'ms')\n",
    "initial_df_bitget.set_index('Date', inplace = True)\n",
    "initial_df_bitget = initial_df_bitget.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f441e0-ac3c-4338-aed1-2f2f09a162f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = pd.concat([initial_df_binance,initial_df_bitget])\n",
    "initial_df = initial_df.sort_index()\n",
    "initial_df = initial_df.drop_duplicates(subset = 'Time')\n",
    "\n",
    "diff = initial_df['Time'].diff().iloc[1]  \n",
    "missing = (initial_df['Time'].diff() != diff).sum() - 1 \n",
    "#If missing = 0, there is no data missing, if missing != 0, there is at least one hour of data missing, probably more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1090c-9a1e-4417-8f6a-6e610c44f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 114)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4ca79-9690-49de-b354-c4d4fab799ad",
   "metadata": {},
   "source": [
    "##### Data transformation and statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff85b0-e267-48ba-a680-4cd490eb2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_df = initial_df.iloc[9:]\n",
    "#The initial_df starts at 23:00, I want it to start at 08:00.\n",
    "df_1h = initial_df.copy()\n",
    "df_4h = initial_df.resample('4h', origin = 'start').first().copy()\n",
    "df_6h = initial_df.resample('6h', origin = 'start').first().copy()\n",
    "df_12h = initial_df.resample('12h', origin = 'start').first().copy()\n",
    "df_1d = initial_df.resample('1D', origin = 'start').first().copy()\n",
    "\n",
    "df_1h['returns'] = df_1h['Open'].pct_change()\n",
    "df_4h['returns'] = df_4h['Open'].pct_change()\n",
    "df_6h['returns'] = df_6h['Open'].pct_change()\n",
    "df_12h['returns'] = df_12h['Open'].pct_change()\n",
    "df_1d['returns'] = df_1d['Open'].pct_change()\n",
    "\n",
    "window_1h = [24, 48, 72, 96, 120]\n",
    "window_4h = [24, 30, 36, 42, 48]\n",
    "window_6h = [16, 20, 24, 28, 32]\n",
    "window_12h = [14, 28, 42, 56, 70]\n",
    "window_1d = [7, 14, 30, 60, 90]\n",
    "\n",
    "for window in window_1h:\n",
    "    df_1h[f'Volatility_{window}'] = df_1h['returns'].rolling(window=window).std()\n",
    "    df_1h[f'Log_Volatility_{window}'] = np.log(df_1h[f'Volatility_{window}'])\n",
    "for window in window_4h:\n",
    "    df_4h[f'Volatility_{window}'] = df_4h['returns'].rolling(window=window).std()\n",
    "    df_4h[f'Log_Volatility_{window}'] = np.log(df_4h[f'Volatility_{window}'])\n",
    "for window in window_6h:\n",
    "    df_6h[f'Volatility_{window}'] = df_6h['returns'].rolling(window=window).std()\n",
    "    df_6h[f'Log_Volatility_{window}'] = np.log(df_6h[f'Volatility_{window}'])\n",
    "for window in window_12h:\n",
    "    df_12h[f'Volatility_{window}'] = df_12h['returns'].rolling(window=window).std()\n",
    "    df_12h[f'Log_Volatility_{window}'] = np.log(df_12h[f'Volatility_{window}'])\n",
    "for window in window_1d:\n",
    "    df_1d[f'Volatility_{window}'] = df_1d['returns'].rolling(window=window).std()\n",
    "    df_1d[f'Log_Volatility_{window}'] = np.log(df_1d[f'Volatility_{window}'])\n",
    "\n",
    "df_1h = df_1h.dropna()\n",
    "df_4h = df_4h.dropna()\n",
    "df_6h = df_6h.dropna()\n",
    "df_12h = df_12h.dropna()\n",
    "df_1d = df_1d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b989a4b-5053-4a05-adee-b2aa5b0358f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adf(series):\n",
    "    result = adfuller(series.dropna())  # Drop NaNs before ADF test\n",
    "    return {\n",
    "        'ADF Statistic': result[0],\n",
    "        'p-value': result[1],\n",
    "        'Critical Values': result[4]\n",
    "    }\n",
    "\n",
    "adf_results = {}\n",
    "\n",
    "# List of dataframes and their respective window lists\n",
    "timeframes = {\n",
    "    '1h': (df_1h, window_1h),\n",
    "    '4h': (df_4h, window_4h),\n",
    "    '6h': (df_6h, window_6h),\n",
    "    '12h': (df_12h, window_12h),\n",
    "    '1d': (df_1d, window_1d),\n",
    "}\n",
    "\n",
    "for timeframe, (df, windows) in timeframes.items():\n",
    "    for window in windows:\n",
    "        column = f'Volatility_{window}'\n",
    "        if column in df.columns:\n",
    "            adf_results[f'{timeframe}_{column}'] = calculate_adf(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825aa1af-b348-4632-96fe-7c2af7c8f106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3654c0-682b-4e95-bebb-2c858f33f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1d['Log_Volatility_90'] = np.log(df_1d['Volatility_90']) #Ya lo calcul√© antes... \n",
    "df_1d['Log_Volatility_90_diff'] = df_1d['Log_Volatility_90'].diff()\n",
    "adf_log_diff = calculate_adf(df_1d['Log_Volatility_90_diff'])\n",
    "print(adf_log_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365524df-4286-4b24-8c2c-1249e809e294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframes = [df_1h, df_4h, df_6h, df_12h, df_1d]\n",
    "titles = [\"1 Hour\", \"4 Hour\", \"6 Hour\", \"12 Hour\", \"1 Day\"]\n",
    "window_sets = {\n",
    "    \"1 Hour\": [24, 48, 72, 96, 120],\n",
    "    \"4 Hour\": [24, 30, 36, 42, 48],\n",
    "    \"6 Hour\": [16, 20, 24, 28, 32],\n",
    "    \"12 Hour\": [14, 28, 42, 56, 70],\n",
    "    \"1 Day\": [7, 14, 30, 60, 90],\n",
    "}\n",
    "\n",
    "for df, title in zip(dataframes, titles):\n",
    "    windows = window_sets[title]\n",
    "    \n",
    "    # Loop through each volatility window for the current DataFrame\n",
    "    for window in windows:\n",
    "        column_name = f\"Volatility_{window}\"\n",
    "        \n",
    "        # Create a figure for the current volatility window\n",
    "        fig, ax = plt.subplots(figsize=(10, 4))\n",
    "        \n",
    "        # Plot ACF for the current volatility column\n",
    "        plot_acf(df[column_name].dropna(), lags=100, alpha=0.01, ax=ax)\n",
    "        ax.set_title(f\"ACF of {column_name} periods ({title})\")\n",
    "        ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        # Save the figure as a separate PDF file\n",
    "        plt.savefig(f\"ACF_{title.replace(' ', '_')}_{column_name}.pdf\", format=\"pdf\", dpi=300)      \n",
    "        # Show the plot in the notebook\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "        \n",
    "        # Close the figure to free memory\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb6b17-2a15-435f-b341-ac026d06af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shift(current_hour, lookahead_hours, frequency):\n",
    "    \"\"\"\n",
    "    Calculate the number of rows to shift based on the current hour,\n",
    "    lookahead period, and data frequency.\n",
    "    \n",
    "    Args:\n",
    "    - current_hour (int): The current hour of the row.\n",
    "    - lookahead_hours (int): The target lookahead period in hours.\n",
    "    - frequency (int): The data frequency in hours (e.g., 1 for 1H, 4 for 4H).\n",
    "\n",
    "    Returns:\n",
    "    - int: The number of rows to shift.\n",
    "    \"\"\"\n",
    "    if current_hour < 8:\n",
    "        shift = lookahead_hours + (8 - current_hour)\n",
    "    elif current_hour > 8 and current_hour <= 19:\n",
    "        shift = lookahead_hours - (current_hour - 8)\n",
    "    elif current_hour >= 20:\n",
    "        shift = lookahead_hours + 8 + 24 - current_hour\n",
    "    else:\n",
    "        shift = lookahead_hours\n",
    "    return int(shift / frequency)\n",
    "\n",
    "#General function to create future Open price columns\n",
    "def add_future_open(df, lookahead_hours_list, frequency):\n",
    "    \"\"\"\n",
    "    Add future Open price columns for given lookahead periods.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The DataFrame containing 'Datetime' and 'Open'.\n",
    "    - lookahead_hours_list (list): List of lookahead periods in hours.\n",
    "    - frequency (int): The data frequency in hours (e.g., 1 for 1H, 4 for 4H).\n",
    "    \"\"\"\n",
    "    df['Datetime'] = pd.to_datetime(df['Time'], unit='ms')\n",
    "    for lookahead_hours in lookahead_hours_list:\n",
    "        col_name = f'Open_{lookahead_hours}h'\n",
    "        df[col_name] = np.nan\n",
    "        for i in range(len(df)):\n",
    "            current_hour = df.iloc[i]['Datetime'].hour\n",
    "            n_rows_to_shift = calculate_shift(current_hour, lookahead_hours, frequency)\n",
    "            target_index = i + n_rows_to_shift\n",
    "            if target_index < len(df):\n",
    "                df.loc[df.index[i], col_name] = df.iloc[target_index]['Open']\n",
    "\n",
    "#Apply function to data\n",
    "add_future_open(df_1h, [48, 72, 96], 1)  #1-hour frequency\n",
    "add_future_open(df_4h, [72, 96, 120], 4)  #4-hour frequency\n",
    "add_future_open(df_6h, [96, 120, 144], 6)\n",
    "add_future_open(df_12h, [120, 144, 168], 12)\n",
    "add_future_open(df_1d, [168, 240, 480], 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4097ea-32fb-4514-8085-6b9242b8648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_static_percentiles(df, volatility_col, percentiles):\n",
    "    \"\"\"\n",
    "    Calculate specified static percentiles for a given volatility column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing the data.\n",
    "    - volatility_col (str): Column name to calculate percentiles for.\n",
    "    - percentiles (list): List of percentiles to calculate.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with percentile as key and value as calculated value.\n",
    "    \"\"\"\n",
    "    return {p: np.percentile(df[volatility_col], p) for p in percentiles}\n",
    "\n",
    "# Function to add move and percentile columns\n",
    "def add_move_and_percentile_columns(df, shifts, percentiles, static_percentiles, volatility_col):\n",
    "    \"\"\"\n",
    "    Add columns for percentage moves and percentile-based flags for a specific volatility column.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame to modify.\n",
    "    - shifts (dict): Mapping of move column names to shift periods.\n",
    "    - percentiles (dict): Mapping of percentiles to static values.\n",
    "    - static_percentiles (dict): Static percentile values for the DataFrame.\n",
    "    - volatility_col (str): Column name for volatility.\n",
    "    \"\"\"\n",
    "    for col_name, shift_period in shifts.items():\n",
    "        df[col_name] = ((df['Open'].shift(-shift_period) / df['Open']) - 1) * 100\n",
    "\n",
    "# Define volatility columns for each timeframe\n",
    "volatility_columns = {\n",
    "    '1h': ['Log_Volatility_24', 'Log_Volatility_48', 'Log_Volatility_72', 'Log_Volatility_96', 'Log_Volatility_120'],\n",
    "    '4h': ['Log_Volatility_24', 'Log_Volatility_30', 'Log_Volatility_36', 'Log_Volatility_42', 'Log_Volatility_48'],\n",
    "    '6h': ['Log_Volatility_16', 'Log_Volatility_20', 'Log_Volatility_24', 'Log_Volatility_28', 'Log_Volatility_32'],\n",
    "    '12h': ['Log_Volatility_14', 'Log_Volatility_28', 'Log_Volatility_42', 'Log_Volatility_56', 'Log_Volatility_70'],\n",
    "    '1d': ['Log_Volatility_7', 'Log_Volatility_14', 'Log_Volatility_30', 'Log_Volatility_60', 'Log_Volatility_90']\n",
    "}\n",
    "\n",
    "# Define shifts for each timeframe\n",
    "shifts_dict = {\n",
    "    '1h': {'Move_1': 48, 'Move_2': 72, 'Move_3' : 96},\n",
    "    '4h': {'Move_1': 18, 'Move_2': 24, 'Move_3' : 30},  # 72h/4h = 18 rows, 96h/4h = 24 rows\n",
    "    '6h': {'Move_1': 16, 'Move_2': 20, 'Move_3' : 24},  # 96h/6h = 16 rows, 120h/6h = 20 rows\n",
    "    '12h': {'Move_1': 10, 'Move_2': 12, 'Move_3' : 14}, # 5d/12h = 10 rows, 7d/12h = 14 rows\n",
    "    '1d': {'Move_1': 7, 'Move_2': 10, 'Move_3': 20}  # 7d, 10d, 20d\n",
    "}\n",
    "\n",
    "# Process each DataFrame\n",
    "timeframes = {'1h': df_1h, '4h': df_4h, '6h': df_6h, '12h': df_12h, '1d': df_1d}\n",
    "percentiles = [20, 80]\n",
    "\n",
    "for tf, df in timeframes.items():\n",
    "    volatility_cols = volatility_columns[tf]  # List of volatility columns\n",
    "    shifts = shifts_dict[tf]\n",
    "\n",
    "    for volatility_col in volatility_cols:  # Loop through each volatility column\n",
    "        # Calculate static percentiles for the current column\n",
    "        static_percentiles = calculate_static_percentiles(df, volatility_col, percentiles)\n",
    "\n",
    "        # Add percentile-based columns for the current volatility column\n",
    "        for percentile in percentiles:\n",
    "            col_name = f'{volatility_col}_{percentile}th_percent'\n",
    "            df[col_name] = (df[volatility_col] < static_percentiles[percentile]).astype(int) if percentile == 20 else \\\n",
    "                           (df[volatility_col] > static_percentiles[percentile]).astype(int)\n",
    "\n",
    "        # Add percentage move columns\n",
    "        add_move_and_percentile_columns(df, shifts, percentiles, static_percentiles, volatility_col)\n",
    "\n",
    "    # For timeframes 6h, 12h, and 1d, set 'Date' as index\n",
    "    if tf in ['6h', '12h', '1d']:\n",
    "        df['Date'] = pd.to_datetime(df['Time'], unit='ms')\n",
    "        df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4ac2a-3cee-4ad5-b6b0-90135bb1301c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_percentiles(df, volatility_columns, percentiles=[20, 80]):\n",
    "    percentile_dict = {}\n",
    "    for col in volatility_columns:\n",
    "        percentile_dict[col] = {p: np.percentile(df[col], p) for p in percentiles}\n",
    "    return percentile_dict\n",
    "\n",
    "# Example: for 1-hour timeframe\n",
    "volatility_cols = ['Log_Volatility_24', 'Log_Volatility_120']\n",
    "percentiles = calculate_percentiles(df_1h, volatility_cols, percentiles=[20, 80])\n",
    "\n",
    "def plot_volatility_with_percentiles(df, volatility_columns, percentiles, title, start = None, end = None):\n",
    "    if start and end:\n",
    "        df = df[(df['Datetime'] >= start) & (df['Datetime'] <= end)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for col in volatility_columns:\n",
    "        plt.plot(df['Datetime'], df[col], label=f'{col}', alpha=1)\n",
    "        # Add percentile lines\n",
    "        p20 = percentiles[col][20]\n",
    "        p80 = percentiles[col][80]\n",
    "        plt.axhline(y=p20, color='red', linestyle='--', label=f'{col} 20th Percentile')\n",
    "        plt.axhline(y=p80, color='blue', linestyle='--', label=f'{col} 80th Percentile')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Log Volatility')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot for 1-hour timeframe\n",
    "plot_volatility_with_percentiles(df_1h, volatility_cols, percentiles, \"1-Hour Volatility and Percentiles\", '2020-01-01', '2021-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e6c6e-125c-4190-afd4-9045b54824d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_move_stats(df, move_columns, volatility_columns, percentile_cols):\n",
    "    \"\"\"\n",
    "    Compute mean and std for move columns based on 20th and 80th percentiles for each volatility window.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The DataFrame containing the data.\n",
    "    - move_columns (list): List of move columns (e.g., ['Move_1', 'Move_2', 'Move_3']).\n",
    "    - volatility_columns (list): List of volatility columns to analyze.\n",
    "    - percentile_cols (dict): Dictionary mapping volatility columns to their 20th and 80th percentile columns.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame summarizing the mean and std for each condition.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for vol_col in volatility_columns:\n",
    "        for move_col in move_columns:\n",
    "            # Filter based on 20th and 80th percentiles\n",
    "            is_below_20th = (df[percentile_cols[vol_col]['20th']] == 1)\n",
    "            is_above_80th = (df[percentile_cols[vol_col]['80th']] == 1)\n",
    "\n",
    "            # Compute statistics for 20th percentile\n",
    "            mean_20th = df.loc[is_below_20th, move_col].mean()\n",
    "            std_20th = df.loc[is_below_20th, move_col].std()\n",
    "\n",
    "            # Compute statistics for 80th percentile\n",
    "            mean_80th = df.loc[is_above_80th, move_col].mean()\n",
    "            std_80th = df.loc[is_above_80th, move_col].std()\n",
    "\n",
    "            # Append results\n",
    "            results.append({\n",
    "                'Volatility Window': vol_col,\n",
    "                'Move Column': move_col,\n",
    "                'Percentile': '20th',\n",
    "                'Mean': mean_20th,\n",
    "                'Std': std_20th\n",
    "            })\n",
    "            results.append({\n",
    "                'Volatility Window': vol_col,\n",
    "                'Move Column': move_col,\n",
    "                'Percentile': '80th',\n",
    "                'Mean': mean_80th,\n",
    "                'Std': std_80th\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define columns and percentiles\n",
    "move_columns = ['Move_1', 'Move_2', 'Move_3']\n",
    "volatility_columns_1h = ['Log_Volatility_24', 'Log_Volatility_48', 'Log_Volatility_72', 'Log_Volatility_96', 'Log_Volatility_120']\n",
    "volatility_columns_4h = ['Log_Volatility_24', 'Log_Volatility_30', 'Log_Volatility_36', 'Log_Volatility_42', 'Log_Volatility_48']\n",
    "volatility_columns_6h = ['Log_Volatility_16', 'Log_Volatility_20', 'Log_Volatility_24', 'Log_Volatility_28', 'Log_Volatility_32']\n",
    "volatility_columns_12h = ['Log_Volatility_14', 'Log_Volatility_28', 'Log_Volatility_42', 'Log_Volatility_56', 'Log_Volatility_70']\n",
    "volatility_columns_1d = ['Log_Volatility_7', 'Log_Volatility_14', 'Log_Volatility_30', 'Log_Volatility_60', 'Log_Volatility_90']\n",
    "percentile_cols_1h = {\n",
    "    col: {\n",
    "        '20th': f'{col}_20th_percent',\n",
    "        '80th': f'{col}_80th_percent'\n",
    "    } for col in volatility_columns_1h\n",
    "}\n",
    "percentile_cols_4h = {\n",
    "    col: {\n",
    "        '20th': f'{col}_20th_percent',\n",
    "        '80th': f'{col}_80th_percent'\n",
    "    } for col in volatility_columns_4h\n",
    "}\n",
    "percentile_cols_6h = {\n",
    "    col: {\n",
    "        '20th': f'{col}_20th_percent',\n",
    "        '80th': f'{col}_80th_percent'\n",
    "    } for col in volatility_columns_6h\n",
    "}\n",
    "percentile_cols_12h = {\n",
    "    col: {\n",
    "        '20th': f'{col}_20th_percent',\n",
    "        '80th': f'{col}_80th_percent'\n",
    "    } for col in volatility_columns_12h\n",
    "}\n",
    "percentile_cols_1d = {\n",
    "    col: {\n",
    "        '20th': f'{col}_20th_percent',\n",
    "        '80th': f'{col}_80th_percent'\n",
    "    } for col in volatility_columns_1d\n",
    "}\n",
    "\n",
    "# Compute stats\n",
    "stats_df_1h = compute_move_stats(df_1h, move_columns, volatility_columns_1h, percentile_cols_1h)\n",
    "stats_df_4h = compute_move_stats(df_4h, move_columns, volatility_columns_4h, percentile_cols_4h)\n",
    "stats_df_6h = compute_move_stats(df_6h, move_columns, volatility_columns_6h, percentile_cols_6h)\n",
    "stats_df_12h = compute_move_stats(df_12h, move_columns, volatility_columns_12h, percentile_cols_12h)\n",
    "stats_df_1d = compute_move_stats(df_1d, move_columns, volatility_columns_1d, percentile_cols_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a8231-d0ad-400f-a704-4b60f844cb38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_df_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdcd861-3c54-4a2e-85db-f12c4198751e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for move in ['Move_1', 'Move_2', 'Move_3']:\n",
    "    subset = stats_df_1h[stats_df_1h['Move Column'] == move]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=subset, x='Volatility Window', y='Std', hue='Percentile')\n",
    "    plt.title(f'Std {move} for 20th and 80th Percentiles')\n",
    "    plt.ylabel('Std')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221bf26-c418-44ed-a9a5-33ea619e5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeline(df, volatility_col, move_col, percentile_col_20th, percentile_col_80th, title):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot the move column\n",
    "    plt.plot(df.index, df[move_col], label=f'{move_col}', alpha=0.7)\n",
    "\n",
    "    # Highlight 20th percentile occurrences\n",
    "    plt.scatter(\n",
    "        df.index[df[percentile_col_20th] == 1], \n",
    "        df.loc[df[percentile_col_20th] == 1, move_col],\n",
    "        color='green', label='20th Percentile', alpha=0.6, marker='o'\n",
    "    )\n",
    "\n",
    "    # Highlight 80th percentile occurrences\n",
    "    plt.scatter(\n",
    "        df.index[df[percentile_col_80th] == 1], \n",
    "        df.loc[df[percentile_col_80th] == 1, move_col],\n",
    "        color='red', label='80th Percentile', alpha=0.6, marker='x'\n",
    "    )\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'{move_col}')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage for 'Log_Volatility_24' and 'Move_1'\n",
    "plot_timeline(\n",
    "    df_1d, \n",
    "    volatility_col='Log_Volatility_7', \n",
    "    move_col='Move_1',\n",
    "    percentile_col_20th='Log_Volatility_7_20th_percent',\n",
    "    percentile_col_80th='Log_Volatility_7_80th_percent',\n",
    "    title='Timeline: Move_1 vs. 20th and 80th Percentile (Log_Volatility_7 - 1d)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1854e1-59df-4442-bbef-fd20932e37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1h[['Move_1', 'Move_2', 'Move_3']] = df_1h[['Move_1', 'Move_2', 'Move_3']].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ef6cff-3536-487c-abcf-97bed94810e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_with_std(df, move_columns, vol_percentiles):\n",
    "    \"\"\"\n",
    "    Perform t-tests between 20th and 80th percentile groups for multiple move columns,\n",
    "    and include standard deviation in the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        move_columns (list): List of move column names to test.\n",
    "        vol_percentiles (dict): Dictionary mapping volatility columns to their percentile columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with t-test results and additional statistics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for move_col in move_columns:\n",
    "        # Filter groups based on 20th and 80th percentiles\n",
    "        below_20th = df[df[vol_percentiles['20th']] == 1][move_col].dropna()\n",
    "        above_80th = df[df[vol_percentiles['80th']] == 1][move_col].dropna()\n",
    "\n",
    "        # Perform t-test\n",
    "        t_stat, p_value = ttest_ind(below_20th, above_80th, equal_var=False)\n",
    "\n",
    "        # Compute statistics\n",
    "        below_20th_mean = below_20th.mean()\n",
    "        above_80th_mean = above_80th.mean()\n",
    "        below_20th_std = below_20th.std()\n",
    "        above_80th_std = above_80th.std()\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Move Column\": move_col,\n",
    "            \"t-statistic\": t_stat,\n",
    "            \"p-value\": p_value,\n",
    "            \"Group 20th Mean\": below_20th_mean,\n",
    "            \"Group 80th Mean\": above_80th_mean,\n",
    "            \"Group 20th Std\": below_20th_std,\n",
    "            \"Group 80th Std\": above_80th_std,\n",
    "            \"Group 20th Count\": len(below_20th),\n",
    "            \"Group 80th Count\": len(above_80th),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "percentile_cols = {\"20th\": 'Log_Volatility_24_20th_percent', \"80th\": 'Log_Volatility_24_80th_percent'}\n",
    "move_columns = [\"Move_1\", \"Move_2\", \"Move_3\"]\n",
    "\n",
    "t_test_results = t_test_with_std(df_1h, move_columns, percentile_cols)\n",
    "t_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8309e8-d1c4-4dc1-9d36-2d443e01896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_std_with_levene(df, move_columns, vol_percentiles):\n",
    "    \"\"\"\n",
    "    Perform Levene's test to compare standard deviations between 20th and 80th percentile groups.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        move_columns (list): List of move column names to test.\n",
    "        vol_percentiles (dict): Dictionary mapping volatility columns to their percentile columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with Levene's test results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for move_col in move_columns:\n",
    "        # Filter groups based on 20th and 80th percentiles\n",
    "        below_20th = df[df[vol_percentiles['20th']] == 1][move_col].dropna()\n",
    "        above_80th = df[df[vol_percentiles['80th']] == 1][move_col].dropna()\n",
    "\n",
    "        # Perform Levene's test\n",
    "        stat, p_value = levene(below_20th, above_80th)\n",
    "\n",
    "        # Compute statistics\n",
    "        below_20th_std = below_20th.std()\n",
    "        above_80th_std = above_80th.std()\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"Move Column\": move_col,\n",
    "            \"Levene Stat\": stat,\n",
    "            \"p-value\": p_value,\n",
    "            \"Group 20th Std\": below_20th_std,\n",
    "            \"Group 80th Std\": above_80th_std,\n",
    "            \"Group 20th Count\": len(below_20th),\n",
    "            \"Group 80th Count\": len(above_80th),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "percentile_cols = {\"20th\": 'Log_Volatility_72_20th_percent', \"80th\": 'Log_Volatility_72_80th_percent'}\n",
    "move_columns = [\"Move_1\", \"Move_2\", \"Move_3\"]\n",
    "\n",
    "levene_results = compare_std_with_levene(df_1h, move_columns, percentile_cols)\n",
    "levene_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5839b4-bdc5-4671-8f82-67d3ff55cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df_1h['Log_Volatility_24'].corr(df_1h['Move_1'])\n",
    "print(f'Correlation between Log_Volatility_24 and Move_1: {correlation:.2f}')\n",
    "\n",
    "X = df_1h['Log_Volatility_24'].values.reshape(-1, 1)  # Predictor\n",
    "y = df_1h['Move_1'].values  # Target\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions and R^2 score\n",
    "y_pred = model.predict(X)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f'Linear Regression R^2 Score: {r2:.2f}')\n",
    "print(f'Coefficient: {model.coef_[0]:.2f}, Intercept: {model.intercept_:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b751f4-6be6-4dbe-a3a7-0442619f2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame combining all volatility data\n",
    "all_log_volatility = pd.DataFrame({\n",
    "    'Volatility': pd.concat([\n",
    "        df_1d['Volatility_7'], df_1d['Volatility_14'],\n",
    "        df_1d['Volatility_30'], df_1d['Volatility_60'],\n",
    "        df_1d['Volatility_90']\n",
    "    ]),\n",
    "    'Category': ['1d_7'] * len(df_1d) + ['1d_14'] * len(df_1d) +\n",
    "                ['1d_30'] * len(df_1d) + ['1d_60'] * len(df_1d) +\n",
    "                ['1d_90'] * len(df_1d)\n",
    "})\n",
    "\n",
    "# Plot KDE with specified order and fill\n",
    "sns.kdeplot(data=all_log_volatility, x='Volatility', hue='Category', palette=['blue', 'red', 'green', 'purple', 'yellow'], fill=True, alpha = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddb5fe-cafc-4cfe-ad23-f02b15884ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame combining all volatility data\n",
    "all_log_volatility = pd.DataFrame({\n",
    "    'Volatility': pd.concat([\n",
    "        df_1d['Log_Volatility_7'], df_1d['Log_Volatility_14'],\n",
    "        df_1d['Log_Volatility_30'], df_1d['Log_Volatility_60'],\n",
    "        df_1d['Log_Volatility_90']\n",
    "    ]),\n",
    "    'Category': ['1d_7'] * len(df_1d) + ['1d_14'] * len(df_1d) +\n",
    "                ['1d_30'] * len(df_1d) + ['1d_60'] * len(df_1d) +\n",
    "                ['1d_90'] * len(df_1d)\n",
    "})\n",
    "\n",
    "# Plot KDE with specified order and fill\n",
    "sns.kdeplot(data=all_log_volatility, x='Volatility', hue='Category', palette=['blue', 'red', 'green', 'purple', 'yellow'], fill=True, alpha = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf25655e-1cda-4a06-9e14-55d0799ae45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5a9d1-4300-4e03-b404-559eb28e3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_state(row, col_20th, col_80th):\n",
    "    if row[col_20th] == 1:\n",
    "        return 0  # Low\n",
    "    elif row[col_80th] == 1:\n",
    "        return 2  # High\n",
    "    else:\n",
    "        return 1  # Middle\n",
    "\n",
    "df_dict = {\n",
    "    'df_1h' : {'df' : df_1h, \n",
    "               'col_20th' : [df_1h['Log_Volatility_24_20th_percent'], df_1h['Log_Volatility_48_20th_percent'], df_1h['Log_Volatility_72_20th_percent'], df_1h['Log_Volatility_96_20th_percent'], df_1h['Log_Volatility_120_20th_percent']], \n",
    "               'col_80th' : [df_1h['Log_Volatility_24_80th_percent'], df_1h['Log_Volatility_48_80th_percent'], df_1h['Log_Volatility_72_80th_percent'], df_1h['Log_Volatility_96_80th_percent'], df_1h['Log_Volatility_120_80th_percent'], ]}, \n",
    "    'df_4h' : {'df' : df_4h, \n",
    "               'col_20th' : [df_4h['Log_Volatility_24_20th_percent'], df_4h['Log_Volatility_30_20th_percent'], df_4h['Log_Volatility_36_20th_percent'], df_4h['Log_Volatility_42_20th_percent'], df_4h['Log_Volatility_48_20th_percent']], \n",
    "               'col_80th' : [df_4h['Log_Volatility_24_80th_percent'], df_4h['Log_Volatility_30_80th_percent'], df_4h['Log_Volatility_36_80th_percent'], df_4h['Log_Volatility_42_80th_percent'], df_4h['Log_Volatility_48_80th_percent'], ]}, \n",
    "    'df_6h' : {'df' : df_6h, \n",
    "               'col_20th' : [df_6h['Log_Volatility_16_20th_percent'], df_6h['Log_Volatility_20_20th_percent'], df_6h['Log_Volatility_24_20th_percent'], df_6h['Log_Volatility_28_20th_percent'], df_6h['Log_Volatility_32_20th_percent']], \n",
    "               'col_80th' : [df_6h['Log_Volatility_16_80th_percent'], df_6h['Log_Volatility_20_80th_percent'], df_6h['Log_Volatility_24_80th_percent'], df_6h['Log_Volatility_28_80th_percent'], df_6h['Log_Volatility_32_80th_percent'], ]}, \n",
    "    'df_12h' : {'df' : df_12h, \n",
    "                'col_20th' : [df_12h['Log_Volatility_14_20th_percent'], df_12h['Log_Volatility_28_20th_percent'], df_12h['Log_Volatility_42_20th_percent'], df_12h['Log_Volatility_56_20th_percent'], df_12h['Log_Volatility_70_20th_percent']], \n",
    "                'col_80th' : [df_12h['Log_Volatility_14_80th_percent'], df_12h['Log_Volatility_28_80th_percent'], df_12h['Log_Volatility_42_80th_percent'], df_12h['Log_Volatility_56_80th_percent'], df_12h['Log_Volatility_70_80th_percent'], ]}, \n",
    "    'df_1d' : {'df' : df_1d, \n",
    "               'col_20th' : [df_1d['Log_Volatility_7_20th_percent'], df_1d['Log_Volatility_14_20th_percent'], df_1d['Log_Volatility_30_20th_percent'], df_1d['Log_Volatility_60_20th_percent'], df_1d['Log_Volatility_90_20th_percent']], \n",
    "               'col_80th' : [df_1d['Log_Volatility_7_80th_percent'], df_1d['Log_Volatility_14_80th_percent'], df_1d['Log_Volatility_30_80th_percent'], df_1d['Log_Volatility_60_80th_percent'], df_1d['Log_Volatility_90_80th_percent'], ]}\n",
    "}\n",
    "    \n",
    "for timeframe, data in df_dict.items():\n",
    "    df = data['df']  # Get the dataframe\n",
    "    col_20th_list = data['col_20th']  # List of 20th percentile columns\n",
    "    col_80th_list = data['col_80th']  # List of 80th percentile columns\n",
    "\n",
    "    # Loop through each period (assumes col_20th and col_80th are aligned)\n",
    "    for i, (col_20th, col_80th) in enumerate(zip(col_20th_list, col_80th_list)):\n",
    "        # Generate column name for the state\n",
    "        state_col = f'State_{i+1}_{timeframe}'\n",
    "        \n",
    "        # Apply the function to assign states\n",
    "        df[state_col] = df.apply(assign_state, axis=1, args=(col_20th.name, col_80th.name))\n",
    "\n",
    "    # Debugging step: Check newly added state columns\n",
    "    print(f\"Timeframe: {timeframe}, State columns: {[f'State_{i+1}_{timeframe}' for i in range(len(col_20th_list))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365a256-e123-4763-9250-ec9ce3e73fc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=4)\n",
    "def calculate_transition_matrix(states, num_states=3):\n",
    "    counts = np.zeros((num_states, num_states))\n",
    "    for i in range(len(states) - 1):\n",
    "        counts[states[i], states[i + 1]] += 1\n",
    "    # Normalize to get probabilities\n",
    "    transition_matrix = counts / counts.sum(axis=1, keepdims=True)\n",
    "    return transition_matrix\n",
    "\n",
    "transition_matrixes = []\n",
    "for timeframe, data in df_dict.items():\n",
    "    df = data['df']  # Get the dataframe\n",
    "\n",
    "    # Loop through each state column\n",
    "    for i in range(len(data['col_20th'])):\n",
    "        state_col = f'State_{i+1}_{timeframe}'\n",
    "        \n",
    "        # Drop NaN values and calculate transition matrix\n",
    "        states = df[state_col].dropna().astype(int).to_list()\n",
    "        transition_matrix = calculate_transition_matrix(states)\n",
    "        transition_matrixes.append(transition_matrix)\n",
    "        \n",
    "        # Print or store the transition matrix\n",
    "        print(f\"Transition Matrix for {state_col}:\")\n",
    "        print(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c7731-7460-42ad-9666-405c74f85e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_transition_matrix(matrix, title):\n",
    "    sns.heatmap(matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", xticklabels=['Low', 'Middle', 'High'], yticklabels=['Low', 'Middle', 'High'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Next State\")\n",
    "    plt.ylabel(\"Current State\")\n",
    "    plt.show()\n",
    "\n",
    "for timeframe, data in df_dict.items():\n",
    "    df = data['df']  # Get the dataframe\n",
    "\n",
    "    for i in range(len(data['col_20th'])):\n",
    "        state_col = f'State_{i+1}_{timeframe}'\n",
    "        \n",
    "        # Drop NaN values and calculate transition matrix\n",
    "        states = df[state_col].dropna().astype(int).to_list()\n",
    "        transition_matrix = calculate_transition_matrix(states)\n",
    "        \n",
    "        # Plot the transition matrix\n",
    "        plot_transition_matrix(transition_matrix, f\"Transition Matrix for {state_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8578b0-6248-49ea-8ba4-6ef3970b7b05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_stationary_distribution(transition_matrix):\n",
    "    n = transition_matrix.shape[0]\n",
    "    \n",
    "    # Subtract identity matrix from transition matrix\n",
    "    A = np.transpose(transition_matrix) - np.eye(n)\n",
    "    \n",
    "    # Add the constraint: sum of probabilities = 1\n",
    "    A[-1, :] = 1  # Replace the last row with ones\n",
    "    b = np.zeros(n)\n",
    "    b[-1] = 1  # Corresponding to the sum constraint\n",
    "    \n",
    "    # Solve the system of linear equations\n",
    "    stationary_distribution = np.linalg.solve(A, b)\n",
    "    return stationary_distribution\n",
    "\n",
    "stationary_distributions = []\n",
    "for matrix in range(len(transition_matrixes)):\n",
    "    transition_matrix = transition_matrixes[matrix]\n",
    "    stationary_distribution = calculate_stationary_distribution(transition_matrix)\n",
    "    stationary_distributions.append(stationary_distribution)\n",
    "    print(\"Stationary Distribution:\", stationary_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46074ed2-b695-44e8-887a-6bf7a72e2253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transition matrix\n",
    "transition_matrix = [\n",
    "    [0.986, 0.014, 0.000],  # From Low\n",
    "    [0.005, 0.992, 0.003],  # From Middle\n",
    "    [0.000, 0.009, 0.991]   # From High\n",
    "]\n",
    "\n",
    "states = ['Low', 'Middle', 'High']\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges with weights\n",
    "for i, state_from in enumerate(states):\n",
    "    for j, state_to in enumerate(states):\n",
    "        weight = transition_matrix[i][j]\n",
    "        if weight > 0:  # Only add edges with non-zero probabilities\n",
    "            G.add_edge(state_from, state_to, weight=weight)\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.circular_layout(G)  # Arrange nodes in a circle\n",
    "plt.figure(figsize=(8, 6))\n",
    "edges = G.edges(data=True)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='lightblue')\n",
    "\n",
    "# Draw edges with width based on weight\n",
    "nx.draw_networkx_edges(G, pos, edgelist=edges, \n",
    "                       arrowstyle='-|>', arrowsize=20,\n",
    "                       width=[d['weight']*10 for _, _, d in edges])\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=14, font_color='black')\n",
    "edge_labels = {(u, v): f\"{d['weight']:.2f}\" for u, v, d in edges}\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=12)\n",
    "\n",
    "plt.title(\"State Transition Diagram\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111ce53-a9db-4e9a-82cb-d8b6b7059f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_contingency_table(df1, state_col1, df2, state_col2):\n",
    "    \"\"\"\n",
    "    Create a contingency table between two state columns from two timeframes.\n",
    "    \"\"\"\n",
    "    merged = pd.DataFrame({\n",
    "        'State_TF1': df1[state_col1],\n",
    "        'State_TF2': df2[state_col2]\n",
    "    }).dropna()\n",
    "    \n",
    "    contingency_table = pd.crosstab(merged['State_TF1'], merged['State_TF2'])\n",
    "    return contingency_table\n",
    "\n",
    "def calculate_transition_heatmap(df1, state_col1, df2, state_col2, tf1_name, tf2_name, state_idx):\n",
    "    \"\"\"\n",
    "    Compute the heatmap of transition probabilities.\n",
    "    \"\"\"\n",
    "    contingency_table = calculate_contingency_table(df1, state_col1, df2, state_col2)\n",
    "    \n",
    "    # Normalize by rows to get transition probabilities\n",
    "    transition_matrix = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
    "    tr_tf_matrix.append(transition_matrix)\n",
    "    \n",
    "    # Heatmap visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(transition_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar_kws={'label': 'Transition Probability'})\n",
    "    plt.title(f\"Transition Heatmap: State_{state_idx+1} {tf1_name} to {tf2_name}\")\n",
    "    plt.xlabel(f\"{tf2_name} States\")\n",
    "    plt.ylabel(f\"{tf1_name} States\")\n",
    "    plt.show()\n",
    "\n",
    "    return contingency_table\n",
    "\n",
    "def cross_timeframe_analysis(df_dict):\n",
    "    \"\"\"\n",
    "    Perform cross-timeframe analysis for all timeframes and their state columns.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    timeframes = list(df_dict.keys())\n",
    "    \n",
    "    # Iterate over pairs of timeframes\n",
    "    for i in range(len(timeframes)):\n",
    "        for j in range(i + 1, len(timeframes)):  # Avoid redundant pairs and self-pairing\n",
    "            tf1_name, tf2_name = timeframes[i], timeframes[j]\n",
    "            df1, df2 = df_dict[tf1_name]['df'], df_dict[tf2_name]['df']\n",
    "            \n",
    "            # Iterate over state columns in the current timeframe\n",
    "            for state_idx, (col_20th_1, col_20th_2) in enumerate(zip(df_dict[tf1_name]['col_20th'], df_dict[tf2_name]['col_20th'])):\n",
    "                # Derive state column names\n",
    "                state_col_1 = f\"State_{state_idx + 1}_{tf1_name}\"\n",
    "                state_col_2 = f\"State_{state_idx + 1}_{tf2_name}\"\n",
    "                \n",
    "                # Calculate heatmap and store results\n",
    "                contingency_table = calculate_transition_heatmap(df1, state_col_1, df2, state_col_2, tf1_name, tf2_name, state_idx)\n",
    "                \n",
    "                # Chi-Square Test for correlation\n",
    "                chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "                results.append({\n",
    "                    \"TF1\": tf1_name,\n",
    "                    \"TF2\": tf2_name,\n",
    "                    \"State_Index\": state_idx + 1,\n",
    "                    \"Chi-Square\": chi2,\n",
    "                    \"p-value\": p\n",
    "                })\n",
    "    \n",
    "    # Convert results to a DataFrame for summary\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "tr_tf_matrix = []\n",
    "# Example Usage\n",
    "results_df = cross_timeframe_analysis(df_dict)\n",
    "\n",
    "# Print summary of results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7afaf0-1036-4d1c-aee9-1333006ba822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Transition matrixes: {transition_matrixes}')\n",
    "print(f'Ns lo q es: {tr_tf_matrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe78e91b-a36d-435b-b7a5-e563d303cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tr_tf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e0efb-df37-4693-a55b-a68cfea01f8f",
   "metadata": {},
   "source": [
    "##### Fetching options data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf8a48d-ffd7-45f7-9266-0d1e58384910",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_data = fetch_options_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b49520-aa81-4bdd-a70b-1c4930ebd442",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b61d7-b5f0-4131-82b9-113e402b3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_data = pd.DataFrame(options_data)\n",
    "options_data.drop(columns = \n",
    "                  ['tick_size_steps','tick_size','settlement_period','settlement_currency',\n",
    "                   'rfq','settlement_currency','price_index','instrument_id',\n",
    "                   'creation_timestamp','counter_currency','contract_size','block_trade_tick_size',\n",
    "                   'block_trade_min_trade_amount','block_trade_commission','base_currency','kind','is_active'],inplace=True) \n",
    "\n",
    "options_data['Date'] = (pd.to_datetime(options_data['expiration_timestamp'], unit = 'ms'))\n",
    "options_data = options_data[options_data['Date'].dt.hour == 8]\n",
    "options_data = options_data[options_data['option_type'] == 'call']\n",
    "options_data['Date'] = options_data['Date'].dt.date\n",
    "#options_data_1h.set_index('Date', inplace = True)\n",
    "\n",
    "options_data.drop(columns = \n",
    "                  ['quote_currency', 'option_type', 'min_trade_amount'],inplace=True) \n",
    "\n",
    "options_data_1h = options_data[options_data['expiration_timestamp'].isin(signal_days_1h_72h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50eaa8c-d9cf-4706-b0ba-7a6cca9eecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1h.loc[:,'Adjusted_date'] = np.where(\n",
    "    df_1h['Datetime'].dt.hour < 20,\n",
    "    df_1h['Datetime'].dt.date,\n",
    "    (df_1h['Datetime'] + pd.Timedelta(days=1)).dt.date\n",
    ")\n",
    "\n",
    "df_1h = df_1h[~df_1h['Adjusted_date'].duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f47008-e2a0-412c-8a22-7d32c0837f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_data_1h.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5278d-5760-426d-a284-e34234335e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1h.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f54580-0bac-45db-b93a-80bc06c71793",
   "metadata": {},
   "outputs": [],
   "source": [
    "butterfly_options = {}\n",
    "\n",
    "for i in range(len(df_1h)):\n",
    "    instrument_names = []\n",
    "    instrument_strikes = []\n",
    "    day = df_1h.iloc[i]['Adjusted_date'] + pd.Timedelta(days=3)\n",
    "    day_open = df_1h.iloc[i]['Open']\n",
    "\n",
    "    matching_rows = options_data_1h[options_data_1h['Date'] == day]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        instrument_names = matching_rows['instrument_name'].tolist()\n",
    "        instrument_strikes = matching_rows['strike'].tolist()\n",
    "\n",
    "        closest_index = min(range(len(instrument_strikes)), key=lambda x: abs(instrument_strikes[x] - day_open))\n",
    "        lowest_index = min(range(len(instrument_strikes)), key=lambda x: abs(instrument_strikes[x] - day_open * 0.95))\n",
    "        highest_index = min(range(len(instrument_strikes)), key=lambda x: abs(instrument_strikes[x] - day_open * 1.05))\n",
    "\n",
    "        closest_name = instrument_names[closest_index]\n",
    "        lowest_name = instrument_names[lowest_index]\n",
    "        highest_name = instrument_names[highest_index]\n",
    "\n",
    "        butterfly_options[day] = {\n",
    "            'lowest' : lowest_name,\n",
    "            'closest' : closest_name,\n",
    "            'highest' : highest_name        \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eaad7-7057-405c-9307-88dfd0f89583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(instrument_names), len(instrument_strikes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
